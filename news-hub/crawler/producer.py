import os
import time, hashlib, json, re
from datetime import datetime, timezone
import feedparser
from bs4 import BeautifulSoup
from kafka import KafkaProducer

# Kafka ì—°ê²°
bootstrap_servers = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:9092")

producer = KafkaProducer(
    bootstrap_servers=[bootstrap_servers],
    value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode("utf-8"),
)

# ----------------------------
# ì–¸ë¡ ì‚¬ë³„ RSS í”¼ë“œ (ì–¸ë¡ ì‚¬, ì¹´í…Œê³ ë¦¬, URL)
# ----------------------------
RSS_FEEDS = [
    ("í•œê²¨ë ˆ", "ì „ì²´", "http://www.hani.co.kr/rss/"),
    ("í•œê²¨ë ˆ", "ì •ì¹˜", "http://www.hani.co.kr/rss/politics/"),
    ("í•œê²¨ë ˆ", "ê²½ì œ", "http://www.hani.co.kr/rss/economy/"),
    ("í•œê²¨ë ˆ", "ì‚¬íšŒ", "http://www.hani.co.kr/rss/society/"),
    ("í•œê²¨ë ˆ", "êµ­ì œ", "http://www.hani.co.kr/rss/international/"),
    ("í•œê²¨ë ˆ", "ë¬¸í™”", "http://www.hani.co.kr/rss/culture/"),
    ("í•œê²¨ë ˆ", "ìŠ¤í¬ì¸ ", "http://www.hani.co.kr/rss/sports/"),
    ("í•œê²¨ë ˆ", "ê³¼í•™", "http://www.hani.co.kr/rss/science/"),
    ("í•œê²¨ë ˆ", "ì˜¤í”¼ë‹ˆì–¸", "http://www.hani.co.kr/rss/opinion/"),

    ("í•œêµ­ê²½ì œ", "ì „ì²´", "https://www.hankyung.com/feed/all-news"),
    ("í•œêµ­ê²½ì œ", "ì •ì¹˜", "https://www.hankyung.com/feed/politics"),
    ("í•œêµ­ê²½ì œ", "ê²½ì œ", "https://www.hankyung.com/feed/economy"),
    ("í•œêµ­ê²½ì œ", "ì‚¬íšŒ", "https://www.hankyung.com/feed/society"),
    ("í•œêµ­ê²½ì œ", "êµ­ì œ", "https://www.hankyung.com/feed/international"),
    ("í•œêµ­ê²½ì œ", "IT", "https://www.hankyung.com/feed/it"),
    ("í•œêµ­ê²½ì œ", "ì˜¤í”¼ë‹ˆì–¸", "https://www.hankyung.com/feed/opinion"),

    ("ë§¤ì¼ê²½ì œ", "ê²½ì œ", "https://www.mk.co.kr/rss/30100041/"),
    ("ë§¤ì¼ê²½ì œ", "ì •ì¹˜", "https://www.mk.co.kr/rss/30200030/"),
    ("ë§¤ì¼ê²½ì œ", "ì‚¬íšŒ", "https://www.mk.co.kr/rss/50400012/"),
    ("ë§¤ì¼ê²½ì œ", "êµ­ì œ", "https://www.mk.co.kr/rss/30300018/"),
    ("ë§¤ì¼ê²½ì œ", "ë¬¸í™”", "https://www.mk.co.kr/rss/30000023/"),
    ("ë§¤ì¼ê²½ì œ", "ìŠ¤í¬ì¸ ", "https://www.mk.co.kr/rss/71000001/"),

    ("ê²½í–¥ì‹ ë¬¸", "ì •ì¹˜", "https://www.khan.co.kr/rss/rssdata/politic_news.xml"),
    ("ê²½í–¥ì‹ ë¬¸", "ê²½ì œ", "https://www.khan.co.kr/rss/rssdata/economy_news.xml"),
    ("ê²½í–¥ì‹ ë¬¸", "ì‚¬íšŒ", "https://www.khan.co.kr/rss/rssdata/society_news.xml"),
    ("ê²½í–¥ì‹ ë¬¸", "êµ­ì œ", "https://www.khan.co.kr/rss/rssdata/kh_world.xml"),
    ("ê²½í–¥ì‹ ë¬¸", "ë¬¸í™”", "https://www.khan.co.kr/rss/rssdata/culture_news.xml"),
    ("ê²½í–¥ì‹ ë¬¸", "ìŠ¤í¬ì¸ ", "http://www.khan.co.kr/rss/rssdata/kh_sports.xml"),
    ("ê²½í–¥ì‹ ë¬¸", "ê³¼í•™", "https://www.khan.co.kr/rss/rssdata/science_news.xml"),
    ("ê²½í–¥ì‹ ë¬¸", "ì˜¤í”¼ë‹ˆì–¸", "https://www.khan.co.kr/rss/rssdata/opinion_news.xml"),
]

# ----------------------------
# ìœ í‹¸ í•¨ìˆ˜
# ----------------------------
def clean_html(html: str) -> str:
    soup = BeautifulSoup(html or "", "html5lib")
    text = soup.get_text(" ")
    return re.sub(r"\s+", " ", text).strip()

def make_id(url: str) -> str:
    return hashlib.sha256(url.encode("utf-8")).hexdigest()

# ë‚šì‹œì„± ê¸°ì‚¬ í•„í„°ë§
BLOCK_KEYWORDS = ["ì¶©ê²©", "í—‰", "ëŒ€ë°•", "ì´ëŸ´ìˆ˜ê°€", "?", "ê³¼ì—°", "ã…‹ã…‹", "ã…ã„·ã„·"]

def is_clickbait(title: str) -> bool:
    for kw in BLOCK_KEYWORDS:
        if kw in title:
            return True
    if re.search(r"â€¦$", title):
        return True
    return False

# ----------------------------
# ë©”ì¸ ë£¨í”„
# ----------------------------
while True:
    for source, category, feed_url in RSS_FEEDS:
        parsed = feedparser.parse(feed_url)
        for e in parsed.entries:
            url = getattr(e, "link", None)
            if not url:
                continue
            title = getattr(e, "title", "")

            # ğŸ”¹ ë‚šì‹œì„± ê¸°ì‚¬ ì œì™¸
            if is_clickbait(title):
                continue

            summary_raw = getattr(e, "summary", "")
            summary = clean_html(summary_raw)
            published = getattr(e, "published", None) or getattr(e, "updated", None)

            doc = {
                "id": make_id(url),
                "title": title,
                "content": summary,
                "summary": summary,
                "url": url,
                "source": source,         # ì–¸ë¡ ì‚¬ëª…
                "category": category,     # ì¹´í…Œê³ ë¦¬ ì¶”ê°€ âœ…
                "source_type": "rss",
                "published_at": published or datetime.now(timezone.utc).isoformat(),
                "tags": [],
                "author": getattr(e, "author", None),
                "lang": parsed.feed.get("language", "und"),
            }

            producer.send("news_topic", {"doc": doc})
            producer.flush()

    time.sleep(60)
